import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model
from datasets import load_dataset

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤ (–º–æ–∂–Ω–æ –ø–æ–º–µ–Ω—è—Ç—å)
MAX_LENGTH = 256

# üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
model_name = "OpenBuddy/openbuddy-mistral-7b-v13"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,   # –ò—Å–ø–æ–ª—å–∑—É–µ–º bf16 –≤–º–µ—Å—Ç–æ 8-bit
    low_cpu_mem_usage=True,         # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è Mac
    device_map={"": 0},             # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ GPU/CPU
    offload_folder="offload"        # –ï—Å–ª–∏ VRAM –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç
)

# model.to("cpu")

# üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç.
# –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ —Ñ–∞–π–ª dataset.jsonl —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Ç—Ä–æ–∫–∏ –≤–∏–¥–∞:
# {"text": "—Å—ä–µ—à—å –µ—â—ë —ç—Ç–∏—Ö —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏—Ö –±—É–ª–æ–∫ –¥–∞ –≤—ã–ø–µ–π —á–∞—é"}
dataset = load_dataset("json", data_files="dataset.jsonl", split="train")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
def preprocess_function(examples):
    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –ø–æ–ª–µ "text" –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è return_tensors,
    # —á—Ç–æ–±—ã dataset.map –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–ª –¥–∞–Ω–Ω—ã–µ.
    tokenized_output = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=MAX_LENGTH
    )
    # –î–ª—è –∑–∞–¥–∞—á–∏ causal LM –º–µ—Ç–∫–∏ (labels) —Ä–∞–≤–Ω—ã input_ids.
    tokenized_output["labels"] = tokenized_output["input_ids"].copy()
    return tokenized_output

# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∫–æ –≤—Å–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É –∏ —É–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ –ø–æ–ª–µ "text"
tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=["text"])

# –ò—Å–ø–æ–ª—å–∑—É–µ–º data collator, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–π –¥–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# üîπ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
lora_config = LoraConfig(
    r=12,                 # default: 8
    lora_alpha=20,        # default: 16
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="all",           # default: none
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

# üîπ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir="./stage-1-finetuned",
    per_device_train_batch_size=1,             # —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è Mac
    gradient_accumulation_steps=8,             # –∏–º–∏—Ç–∏—Ä—É–µ–º –±–æ–ª—å—à–∏–π –±–∞—Ç—á
    num_train_epochs=3,
    save_steps=500,
    logging_dir="./logs",
    fp16=False,                              # –Ω–∞ Mac fp16 –º–æ–∂–µ—Ç –¥–∞–≤–∞—Ç—å –æ—à–∏–±–∫–∏; –∏—Å–ø–æ–ª—å–∑—É–µ–º bf16
    bf16=True,                               # bf16 ‚Äì –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –¥–ª—è Apple Silicon
    optim="adamw_torch",
    remove_unused_columns=False
)

# –°–æ–∑–¥–∞–µ–º Trainer –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
trainer = Trainer(
    model=model,
    train_dataset=tokenized_dataset,
    args=training_args,
    data_collator=data_collator,
)

# üîπ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
trainer.train()
