import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model
from datasets import load_dataset
from transformers import DataCollatorWithPadding

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤ (–º–æ–∂–Ω–æ –ø–æ–º–µ–Ω—è—Ç—å)
MAX_LENGTH = 256

# üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
model_name = "t-tech/T-lite-it-1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)
data_collator = DataCollatorWithPadding(tokenizer, padding=True)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º bf16 –≤–º–µ—Å—Ç–æ 8-bit
    low_cpu_mem_usage=True,        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è Mac
    device_map={"": 0},            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ GPU/CPU
    offload_folder="offload"       # –ï—Å–ª–∏ VRAM –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç
)

# üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç (JSONL —Å –ø–∞—Ä–∞–º–∏ prompt-response)
dataset = load_dataset("json", data_files="dataset.jsonl")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
def preprocess_function(examples):
    texts = []
    for message_list in examples["messages"]:
        if isinstance(message_list, list):
            user_messages = [msg["content"] for msg in message_list if msg.get("role") == "user"]
            texts.append(" ".join(user_messages))  # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç

    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Å padding –∏ truncation
    tokenized_output = tokenizer(
        texts,
        padding="max_length",
        truncation=True,
        max_length=MAX_LENGTH,
        return_tensors="pt"
    )

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–Ω–∑–æ—Ä—ã –≤ —Å–ø–∏—Å–∫–∏ (–µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è) –∏ –¥–æ–±–∞–≤–ª—è–µ–º labels –∫–∞–∫ –∫–æ–ø–∏—é input_ids
    return {
        "input_ids": tokenized_output["input_ids"].tolist(),
        "attention_mask": tokenized_output["attention_mask"].tolist(),
        "labels": tokenized_output["input_ids"].tolist()
    }

# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é
tokenized_datasets = dataset.map(preprocess_function, batched=True)

# –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω—é—é –∫–æ–ª–æ–Ω–∫—É, —á—Ç–æ–±—ã DataCollator –Ω–µ –ø—ã—Ç–∞–ª—Å—è –ø–∞–¥–¥–∏—Ç—å "messages"
tokenized_datasets = tokenized_datasets.remove_columns(["messages"])

# üîπ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
lora_config = LoraConfig(
    r=8, lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

# üîπ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir="./stage-1-finetuned",
    per_device_train_batch_size=1,  # Mac –∏–º–µ–µ—Ç –º–∞–ª–æ VRAM
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    save_steps=500,
    logging_dir="./logs",
    fp16=False,  # fp16 –º–æ–∂–µ—Ç –¥–∞–≤–∞—Ç—å –æ—à–∏–±–∫–∏ –Ω–∞ Mac, –ª—É—á—à–µ bf16
    bf16=True,   # bf16 - –ª—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –¥–ª—è Apple Silicon
    optim="adamw_torch",
    remove_unused_columns=False
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_datasets["train"],
    args=training_args,
    data_collator=data_collator
)

# üîπ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
trainer.train()
