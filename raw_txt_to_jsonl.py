import json
from transformers import AutoTokenizer

# üîπ –ù–∞—Å—Ç—Ä–æ–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Å–≤–æ–π)
model_name = "OpenBuddy/openbuddy-mistral-7b-v13"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# üîπ –õ–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤
MAX_LENGTH = 256

# üîπ –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —É–ø–∞–∫–æ–≤–∫–∏ –≤ batch
def split_text_into_batches(text, max_length):
    sentences = text.split(". ")  # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Ç–æ—á–∫–∞–º
    current_batch = []
    batches = []

    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue

        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—É—â–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        tokenized_sentence = tokenizer(sentence, add_special_tokens=False)
        token_count = len(tokenized_sentence["input_ids"])

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–ª–µ–∑–∞–µ—Ç –ª–∏ –æ–Ω–æ –≤ —Ç–µ–∫—É—â–∏–π batch
        current_tokens = sum(len(tokenizer(s, add_special_tokens=False)["input_ids"]) for s in current_batch)
        if current_tokens + token_count <= max_length:
            current_batch.append(sentence)
        else:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–æ–∫—É –∏ –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é
            batches.append(". ".join(current_batch) + ".")
            current_batch = [sentence]

    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∫—É—Å–æ–∫
    if current_batch:
        batches.append(". ".join(current_batch) + ".")

    return batches

# üîπ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª
input_file = "raw_texts.txt"
output_file = "dataset.jsonl"

with open(input_file, "r", encoding="utf-8") as f, open(output_file, "w", encoding="utf-8") as out_f:
    for line in f:
        line = line.strip()
        if not line:
            continue

        batches = split_text_into_batches(line, MAX_LENGTH)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSONL-—Ñ–æ—Ä–º–∞—Ç–µ
        for batch in batches:
            json.dump({"text": batch}, out_f, ensure_ascii=False)
            out_f.write("\n")

print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {output_file}")
